{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üì• Importing Libraries","metadata":{}},{"cell_type":"markdown","source":"Data Mining Assignment 2 \n\nAarona Preethee P\n\n1002029616 ","metadata":{}},{"cell_type":"code","source":"import re\nimport json\nimport string\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import  CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:12:41.802828Z","iopub.execute_input":"2022-04-19T02:12:41.803100Z","iopub.status.idle":"2022-04-19T02:12:43.230435Z","shell.execute_reply.started":"2022-04-19T02:12:41.803032Z","shell.execute_reply":"2022-04-19T02:12:43.229772Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# üóÉÔ∏è Load Dataset","metadata":{}},{"cell_type":"code","source":"path ='../input/news-category-dataset/News_Category_Dataset_v2.json'","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:12:50.879307Z","iopub.execute_input":"2022-04-19T02:12:50.879827Z","iopub.status.idle":"2022-04-19T02:12:50.883734Z","shell.execute_reply.started":"2022-04-19T02:12:50.879789Z","shell.execute_reply":"2022-04-19T02:12:50.882823Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"list_ = []\nwith open(path) as files:\n    for file in files:\n        list_.append(json.loads(file))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:01.545387Z","iopub.execute_input":"2022-04-19T02:13:01.545631Z","iopub.status.idle":"2022-04-19T02:13:03.394116Z","shell.execute_reply.started":"2022-04-19T02:13:01.545608Z","shell.execute_reply":"2022-04-19T02:13:03.393378Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# üìù Meta information of Dataframe","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(list_)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:07.903167Z","iopub.execute_input":"2022-04-19T02:13:07.903490Z","iopub.status.idle":"2022-04-19T02:13:08.225217Z","shell.execute_reply.started":"2022-04-19T02:13:07.903436Z","shell.execute_reply":"2022-04-19T02:13:08.223593Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:11.079589Z","iopub.execute_input":"2022-04-19T02:13:11.080210Z","iopub.status.idle":"2022-04-19T02:13:11.251448Z","shell.execute_reply.started":"2022-04-19T02:13:11.080175Z","shell.execute_reply":"2022-04-19T02:13:11.250788Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# üîé Checking for NaN values","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:14.520871Z","iopub.execute_input":"2022-04-19T02:13:14.521099Z","iopub.status.idle":"2022-04-19T02:13:14.676866Z","shell.execute_reply.started":"2022-04-19T02:13:14.521075Z","shell.execute_reply":"2022-04-19T02:13:14.676001Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# üî• EDA & Visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\ncount = data.category.value_counts()\nsns.barplot(x=count.index, y=count)\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:17.563630Z","iopub.execute_input":"2022-04-19T02:13:17.563903Z","iopub.status.idle":"2022-04-19T02:13:18.134259Z","shell.execute_reply.started":"2022-04-19T02:13:17.563875Z","shell.execute_reply":"2022-04-19T02:13:18.133652Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# üßπ Cleaning Data","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()                                  # lower-case all characters\n    text =  re.sub(r'@\\S+', '',text)                     # remove twitter handles\n    text =  re.sub(r'http\\S+', '',text)                  # remove urls\n    text =  re.sub(r'pic.\\S+', '',text) \n    text =  re.sub(r\"[^a-zA-Z+']\", ' ',text)             # only keeps characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')      # keep words with length>1 only\n    text = \"\".join([i for i in text if i not in string.punctuation])\n    words = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')   # remove stopwords\n    text = \" \".join([i for i in words if i not in stopwords and len(i)>2])\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()            # remove repeated/leading/trailing spaces\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:27.429323Z","iopub.execute_input":"2022-04-19T02:13:27.430116Z","iopub.status.idle":"2022-04-19T02:13:27.437295Z","shell.execute_reply.started":"2022-04-19T02:13:27.430064Z","shell.execute_reply":"2022-04-19T02:13:27.436404Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data['Text_cleaning'] = data.headline.apply(clean_text)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:13:31.160663Z","iopub.execute_input":"2022-04-19T02:13:31.161488Z","iopub.status.idle":"2022-04-19T02:14:38.359509Z","shell.execute_reply.started":"2022-04-19T02:13:31.161449Z","shell.execute_reply":"2022-04-19T02:14:38.358990Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer()\ndata_vectorizer = vectorizer.fit_transform(data['Text_cleaning'])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:14:47.338984Z","iopub.execute_input":"2022-04-19T02:14:47.339366Z","iopub.status.idle":"2022-04-19T02:14:49.591294Z","shell.execute_reply.started":"2022-04-19T02:14:47.339321Z","shell.execute_reply":"2022-04-19T02:14:49.590620Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"labels = data['category']","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:15:10.274031Z","iopub.execute_input":"2022-04-19T02:15:10.274314Z","iopub.status.idle":"2022-04-19T02:15:10.277942Z","shell.execute_reply.started":"2022-04-19T02:15:10.274282Z","shell.execute_reply":"2022-04-19T02:15:10.277024Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# ‚úÇÔ∏è Train test split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_vectorizer, labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:15:18.486724Z","iopub.execute_input":"2022-04-19T02:15:18.486986Z","iopub.status.idle":"2022-04-19T02:15:18.536215Z","shell.execute_reply.started":"2022-04-19T02:15:18.486958Z","shell.execute_reply":"2022-04-19T02:15:18.535549Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# üìö Training model","metadata":{}},{"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:15:37.750448Z","iopub.execute_input":"2022-04-19T02:15:37.750753Z","iopub.status.idle":"2022-04-19T02:15:39.803109Z","shell.execute_reply.started":"2022-04-19T02:15:37.750722Z","shell.execute_reply":"2022-04-19T02:15:39.802385Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y_pred = nb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:15:43.306253Z","iopub.execute_input":"2022-04-19T02:15:43.306538Z","iopub.status.idle":"2022-04-19T02:15:43.337416Z","shell.execute_reply.started":"2022-04-19T02:15:43.306508Z","shell.execute_reply":"2022-04-19T02:15:43.336585Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# üß™ Test & Train Accuracy","metadata":{}},{"cell_type":"code","source":"Acc_train = nb.score(X_train, y_train)\nacc_test = nb.score(X_test, y_test)\nprint('Train Accuracy : {:.2f}%'.format(Acc_train*100))\nprint('Test Accuracy  : {:.2f}%'.format(acc_test*100))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:15:46.260110Z","iopub.execute_input":"2022-04-19T02:15:46.260408Z","iopub.status.idle":"2022-04-19T02:15:46.871961Z","shell.execute_reply.started":"2022-04-19T02:15:46.260373Z","shell.execute_reply":"2022-04-19T02:15:46.871040Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# ‚úîÔ∏è Classification report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:16:00.390192Z","iopub.execute_input":"2022-04-19T02:16:00.390796Z","iopub.status.idle":"2022-04-19T02:16:02.183028Z","shell.execute_reply.started":"2022-04-19T02:16:00.390760Z","shell.execute_reply":"2022-04-19T02:16:02.182182Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Vocabulary ","metadata":{}},{"cell_type":"code","source":"vocabulary_list= []\nfor i in data['headline']:\n  sp= i.split(' ')\n  for j in sp:\n    if j not in vocabulary_list:\n      vocabulary_list.append(j)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:16:17.160692Z","iopub.execute_input":"2022-04-19T02:16:17.161439Z","iopub.status.idle":"2022-04-19T02:19:19.599548Z","shell.execute_reply.started":"2022-04-19T02:16:17.161399Z","shell.execute_reply":"2022-04-19T02:19:19.595169Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"vocabulary_list","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:19:23.601646Z","iopub.execute_input":"2022-04-19T02:19:23.601905Z","iopub.status.idle":"2022-04-19T02:19:23.624500Z","shell.execute_reply.started":"2022-04-19T02:19:23.601878Z","shell.execute_reply":"2022-04-19T02:19:23.623703Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Alpha value for smoothing\na = 0.001\n#Calculate probability of each word based on class\npb_ij = df.groupby(['classIdx','wordIdx'])\npb_j = df.groupby(['classIdx'])\nPr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + len(vect.vocabulary_))    \n#Unstack series\nPr = Pr.unstack()\n\n#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\nfor c in range(0,41):\n    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))\n\n#Convert to dictionary for greater speed\nPr_dict = Pr.to_dict()\n\nPr","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:19:19.611067Z","iopub.status.idle":"2022-04-19T02:19:19.611477Z","shell.execute_reply.started":"2022-04-19T02:19:19.611245Z","shell.execute_reply":"2022-04-19T02:19:19.611265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stopwords\nprint(\", \".join(stopwords.words('english')))\nstops = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:56:15.308074Z","iopub.execute_input":"2022-04-19T02:56:15.308607Z","iopub.status.idle":"2022-04-19T02:56:15.324204Z","shell.execute_reply.started":"2022-04-19T02:56:15.308570Z","shell.execute_reply":"2022-04-19T02:56:15.323073Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def calculate_prior_probabilities(df):\n    prior_probabilities = df.groupby(by = 'label').apply(lambda x: len(x)/len(df))\n    return np.log(prior_probabilities).values\ndef calculate_posterior_probabilities(df_row, mean, variance, n_unique_labels, n_cols):\n    posterior_probabilities = []\n  \n  # calculate probabilities wrt each label to find max\n    for i in range(n_unique_labels):\n        posterior = 0\n\n    # for each feature\n    for j in range(n_cols):\n        posterior += np.log(calculate_probability_density(mean[i][j], variance[i][j], df_row[j]))\n    posterior_probabilities.append(posterior)\n\n    return posterior_probabilities","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(nbc,df):\n    x_test = df.drop(columns=['label'])\n    x_test = x_test.to_numpy()\n    predictions = []\n    for i in tqdm.tqdm(range(len(x_test))):\n        prior = nbc['prior_probabilities']\n        posterior = calculate_posterior_probabilities(x_test[i], nbc['mean'], nbc['variance'], nbc['n_unique_labels'], nbc['n_cols'])  # returns log\n        probabilities = prior + posterior\n        # one with max prob will be the output \n        mx_idx = np.argmax(probabilities)\n\n        predictions.append(nbc['unique_labels'][mx_idx])  # add log values\n\n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nbc = fit(dfb_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\npredictions = predict(nbc,dfb_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\nfig, ax = plt.subplots(figsize=(20,20))\n\ndisp = ConfusionMatrixDisplay.from_predictions(dfb_test['label'].to_numpy(),predictions,ax=ax)\nplt.grid(False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = { \n    'alpha': np.arange(0, 1.1, 0.1),\n    'fit_prior': [True, False],\n}\n\nsvc = MultinomialNB()\nnb_clf = GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, # Stratified Kfold by default (cv)\n                      scoring='balanced_accuracy', refit=True,\n                      return_train_score=False, n_jobs=-1, verbose=1)\nnb_clf.fit(X_train_enc, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def general_metrics(y_test, y_pred):\n    print('Accuracy: ' + str(accuracy_score(y_test, y_pred).round(2)))\n    print('Balanced Accuracy: ' + str(balanced_accuracy_score(y_test, y_pred).round(2)))\n    print('Report:')\n    print(classification_report(y_test, y_pred))\n\ndef plot_confusion_matrix(y_test, y_pred, classes):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cmp = ConfusionMatrixDisplay(\n        confusion_matrix(y_test, y_pred),\n        display_labels=classes)\n    cmp.plot(ax=ax)\n    plt.show()\n    \ndef plot_roc_curve(y_test, y_score, classes):\n    n_classes = len(classes)\n    y_test = label_binarize(y_test, classes=np.arange(n_classes))\n\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n  \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure(figsize=(10,5))\n    lw = 2\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n             color=\"tab:pink\", linestyle=\":\", linewidth=4,)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n             color=\"tab:gray\", linestyle=\":\", linewidth=4,)\n\n    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n    for i, color in enumerate(colors):\n        type_class = classes[i]\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw, \n                 label=\"ROC curve of \" + type_class +\" (area = {1:0.2f})\".format(i, roc_auc[i]),)\n\n    plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC curve\")\n    plt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\nhttps://dataaspirant.com/naive-bayes-classifier-machine-learning/ \n\n\nhttps://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/ \n\n\nhttps://www.kaggle.com/datasets/rmisra/news-category-dataset/code (No Contribution)\n\n\nhttps://www.kaggle.com/code/hamzamanssor/news-category-classification \n(No Contribution)\n\nhttps://www.kaggle.com/code/lucasgarciadeviedma/profile-based-retrieval-with-w2vec-and-nn (No Contribution)\n\n\nhttps://www.kaggle.com/code/jaimemorillo/profile-based-retrieval \n\n\nhttps://www.kaggle.com/code/tuhinkumardutta/newscategory-lda-topic-modelling-0-41coherence \n\n\n https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67 \n","metadata":{}},{"cell_type":"markdown","source":"https://machinelearningmastery.com/k-fold-cross-validation/ \n\n\nhttps://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece \n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}